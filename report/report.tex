\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Introduction to Learning and Intelligent Systems - Spring 2015}
\author{jo@student.ethz.ch\\ stegeran@ethz.ch\\sakhadov@student.ethz.ch}
\date{\today}

\begin{document}
\maketitle

\section*{Project 4 : Classification with Missing Labels}

Training data for the last project contained very little labelled samples (80 out of about 40'000). Therefore we decided to use a semi-supervised learning method.

Moreover it was required to not only find out the most probably label, but to find out the actual probability for a new data point that it belongs to a label. Therefore we tried to use the seminb\footnote{http://www.mblondel.org/code/seminb.git} library, which implements the method from the pater "Semi-supervised Text Classification Using EM" \footnote{Semi-supervised Text Classification Using EM, Kamal Nigam, Andrew McCallum, Tom M. Mitchell }. In a first step the classifier is trained with the labeled data. Secondly the expectation step uses the current classifier to find the probability of the label of each unlabeled data point. In a third step the maximization is done, where the classifier is trained with all data including the new probabilistically-labeled one. The second and third step is then repeated until the accuracy is satisfying.
Unfortunatly we were not able to adapt this script to our feature imput.

Experimenting with mixed-gaussian models did not yield very good scores: First we tried the PyMix Python GMM Library. However we did not bring that to work and failed even at the data read-in stage. Sklearn does have a GMM classifier, but it does not have semi-supervised learning builtin. Experimenting with other libraries did not yield any usable results, too.

We then decided to go an entirely different path: to use sklearns LabelPropagation and LabelSpreading implementations. This was not very successfull at the beginning, as curiously most of the predicted Y values were NaN (Not a number). Further Investigation showed that after fitting, the sklearn.LabelSpreading internal state label\_distributions\_ had some values set to NaN, which then propagated to the Ypred values. We choose the solution to just set every NaN to 0 in the internal state. This gave us a score of about 4. Generally we could observe that LabelSpreading gave us much better results than LabelPropagation.

The breaktrough came when we used the Sklearn LabelSpreader with a RBF (Radial basis function) kernel. This bumped our score to 0.71 immediatly.

It is to observe that LabelSpreading with RBF kernel requires quite a lot of memory and CPU power. It ran for about 10 minutes on a Macbooc Pro with 8 cores Intel i7 @ 2.30GHz. The algorithm was not runnable on a 8gb RAM machine.

Another available option is k-Nearest Neighbours function which runs faster and does not need as much memory. However for this project the result had a higher preference than the runtime.

RBF kernel takes gamma as a parameter which we fine tuned with cross-validation.

\end{document}
